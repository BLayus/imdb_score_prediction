# -*- coding: utf-8 -*-
"""Streamlit app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-BzxndyS7eW2Qv06A7irgUDVhOXkkN72

##Bibliotecas
"""

!pip install streamlit

!pip install --upgrade scikit-learn

# Import Streamlit
import streamlit as st

# Import libraries
import pandas as pd
import numpy as np
import re
from datetime import datetime
import datetime as dt
import requests

# Machine Learning Libraries
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, TargetEncoder
from sklearn.preprocessing import FunctionTransformer

from sklearn.pipeline import Pipeline
from sklearn.pipeline import make_pipeline

from sklearn.compose import make_column_transformer

from sklearn.model_selection import train_test_split

from xgboost import XGBRegressor

import pickle

"""## Dataset"""

st.set_page_config( layout= 'wide')

@st.cache_data
def get_data (path):
  df = pd.read_csv(path)
  return df

# get data

path = 'https://raw.githubusercontent.com/BLayus/imdb_score_prediction/main/dataset/Dataset_imdb.csv'
df = get_data(path)

"""##Pre Processing"""

# The 'Apollo 13' movie has no date, in official website, I found that release year was 1995
# Using Loc to find and replace the value

df.loc[df['Released_Year'] == 'PG', 'Released_Year'] = 1995

# Convert Released_Year to datetime year only

def convert_datetime(df):
  df['Released_Year']= pd.to_datetime(df['Released_Year'], format= '%Y', errors= 'coerce').dt.year
  return df

# Converting strings in column "Runtime" to int 64 and removing substring 'min'

def convert_runtime(df):
  if df['Runtime'].dtype == 'object':
    df['Runtime']= df['Runtime'].str.extract('(\d+)', expand=False).astype('int64')
  return df

# Replace commas in Gross strings

def convert_gross(df):
  df['Gross'] = df['Gross'].astype(str)
  df['Gross']= df['Gross'].str.replace(r'[^\w\s]', '', regex= True)

  # Converting gross type to numerical and fill NaN

  df['Gross'] = pd.to_numeric(df['Gross'], errors='coerce').fillna(0).astype('int64')
  return df

# Certificate column, imput missing with mode

def simple_imputer(df):
  imputer= SimpleImputer(missing_values= np.nan, strategy= 'most_frequent')
  data= df[['Certificate']]
  imputer= imputer.fit(data)
  df['Certificate']= imputer.transform(data).flatten()

  # Meta Score column, imput with median

  imputer= SimpleImputer(missing_values= np.nan, strategy= 'median')
  data= df[['Meta_score']]
  imputer= imputer.fit(data)
  df['Meta_score']= imputer.transform(data).flatten()
  return df

# Certificate grouping and converting to numerical info
# As this grouping has low cardinality, we can use One Hot Encoder

def certificate_groups(df):
  df['Certificate']= df['Certificate'].apply(lambda x: 'all_age_group' if x == ['U', 'G', 'Passed', 'Approved']
                                             else 'accompanied_age_group' if x == ['PG', 'TV-PG', 'U/A', 'GP']
                                             else '14_years_group' if x == ['PG-13', 'TV-14']
                                             else '16_years_group' if x == ['16', 'R']
                                             else 'adult_group')
  return df

# Convert column dtypes

def convert_dtypes(df):
  df['Released_Year'] = df['Released_Year'].astype(int)
  df['Gross'] = df['Gross'].astype(int)
  return df

# Drop Unnecessary columns

def drop_cols(df):
  drop_cols= ['Series_Title', 'Overview']
  df.drop(columns= drop_cols, inplace= True)
  return df

# Convert column dtypes

def convert_dtypes(df):
  df['Released_Year'] = df['Released_Year'].astype(int)
  df['Gross'] = df['Gross'].astype(int)
  return df

# Drop Unnecessary columns

def drop_cols(df):
  drop_cols= ['Series_Title', 'Overview']
  df.drop(columns= drop_cols, inplace= True)
  return df

# Create a function with with steps before encoding

def pre_encoder(df):
  convert_datetime(df)
  convert_runtime(df)
  convert_gross(df)
  simple_imputer(df)
  certificate_groups(df)
  drop_cols(df)
  convert_dtypes(df)

  return(df)

# Apply pre process function

pre_encoder(df)

df.sample(3)

# Defining columns to encode

ohe_cols= ['Certificate']

te_cols= ['Released_Year', 'Genre', 'Director', 'Star1', 'Star2', 'Star3', 'Star4']

# Instatiate the encoders

target_enc = TargetEncoder(smooth='auto', target_type='continuous')

ohe_enc = OneHotEncoder(handle_unknown='ignore')

# Making column transformer

col_trans= make_column_transformer(
    (ohe_enc, ohe_cols),
    (target_enc, te_cols),
    remainder= 'passthrough')

# Creating a pipeline to encode

def passthrough_func(X):
    print("Data after ColumnTransformer:", type(X), X.shape)
    df= pd.DataFrame(X)
    for col in df.columns:
      df[col] = df[col].astype(float)
    return df

enc_pipeline= make_pipeline(col_trans,
                        FunctionTransformer(passthrough_func, validate=False)
                        )

"""##Streamlit APP"""

# Create a Streamlit app

st.title("IMDB Rating Prediction API")

st.write("Enter the movie details to predict the IMDB rating:")

# Create a form to input movie data

with st.form('Preencha os Dados'):
    Series_Title = st.text_input('Title')
    Released_Year = st.number_input('Year (YYYY format)')
    Certificate = st.text_input('Certificate')
    Runtime = st.number_input('Runtime (in minutes)')
    Genre = st.text_input('Genre')
    Meta_Score = st.number_input('Meta Score')
    Director = st.selectbox('Director', df['Director'].unique())
    Star_1 = st.selectbox('Star 1', df['Star1'].unique())
    Star_2 = st.selectbox('Star 2', df['Star2'].unique())
    Star_3 = st.selectbox('Star 3', df['Star3'].unique())
    Star_4 = st.selectbox('Star 4', df['Star4'].unique())
    No_of_Votes = st.number_input('No of Votes')
    Gross = st.number_input('Gross')

df_pred= pd.DataFrame({'Released_Year' :[ Released_Year],
                        'Certificate' : [Certificate],
                        'Runtime' : [Runtime],
                        'Genre' : [Genre],
                        'Meta_Score' : [Meta_Score],
                        'Director' : [Director],
                        'Star_1' : [Star_1],
                        'Star_2' : [Star_2],
                        'Star_3' : [Star_3],
                        'Star_4' : [Star_4],
                        'No_of_Votes' : [No_of_Votes],
                        'Gross' : [Gross]
                        })

# Opening saved pickle file

pickle_url = 'https://github.com/BLayus/imdb_score_prediction/raw/main/Model/model_pickle'
response = requests.get(pickle_url)
with open('model_pickle', 'wb') as f:
    f.write(response.content)

with open('model_pickle', 'rb') as f:
    mp = pickle.load(f)

# Create a button to make the prediction

if st.button("Predict IMDB Rating"):

  # Check columns
  for col in df:
    if col not in df_pred:
      df_pred[col] = 1
  else:
    pass
  # Pre process input data point
  pre_encoder(def_pred)

  #  Define X and Target
  x_p= df_pred.drop(columns= ['IMDB_Rating'], axis= 1)
  y_p= df_pred['IMDB_Rating']

  # Applying pipeline
  x_p = enc_pipeline.transform(x_p)

  # Make prediction with pickle
  mp.predict(x_p)